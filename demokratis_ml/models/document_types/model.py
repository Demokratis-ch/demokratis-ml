"""Model construction for document type classification."""

import logging

import numpy as np
import pandas as pd
import sklearn.pipeline
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.ensemble import (
    GradientBoostingClassifier,  # noqa: F401
    RandomForestClassifier,
)
from sklearn.linear_model import LogisticRegression  # noqa: F401
from sklearn.preprocessing import OneHotEncoder, StandardScaler  # noqa: F401

EXTRA_FEATURE_COLUMNS = (
    # PDF features
    "count_pages",  # int: number of pages in the document
    "count_pages_containing_images",  # int: number of pages containing images (for documents <= 50 pages)
    "count_pages_containing_tables",  # int: number of pages containing tables (for documents <= 50 pages)
    "average_page_aspect_ratio",  # float: average aspect ratio of pages (width / height) (for documents <= 50 pages)
    # / Removed: train/test distributions don't match well:
    # \ "fraction_pages_containing_images",  # float: count_pages_containing_images / count_pages
    "fraction_pages_containing_tables",  # float: count_pages_containing_tables / count_pages
    # Keyword-like features
    "contains_synopse_keyword",  # bool: whether the beginning of the document contains the word "synopse"
    "contains_salutation",  # bool: whether the beginning of the document contains a formal German letter greetings
    "contains_table_on_first_page",  # bool: whether the first page contains a table
    # Time features
    "days_after_consultation_start",  # int: days since the consultation started
    "days_after_consultation_end",  # int: days since the consultation ended (often a negative number)
    "consultation_start_timestamp",  # int: timestamp of the consultation start date (in seconds since epoch)
)

EXTRA_CATEGORICAL_COLUMNS = ("is_federal_consultation",)

_NULL_REPLACEMENTS = {
    "count_pages": -1,
    "count_pages_containing_images": -1,
    "count_pages_containing_tables": -1,
    "average_page_aspect_ratio": -1.0,
    "fraction_pages_containing_tables": -1.0,
}

logger = logging.getLogger("document_types.model")


def create_matrices(df: pd.DataFrame, fill_nulls: bool = False) -> tuple[np.ndarray, pd.Series]:
    """Convert a dataframe (the result of preprocessing) into a feature matrix and a target vector."""
    embeddings = np.vstack(df["embedding"])
    x = np.hstack(
        [embeddings]
        + [_pick_column(df, column, fill_nulls) for column in EXTRA_FEATURE_COLUMNS]
        + [
            df[list(EXTRA_CATEGORICAL_COLUMNS)],
        ]
    ).astype(np.float32)
    y = df["document_type"]
    assert x.shape[0] == y.shape[0]
    return x, y


def _pick_column(df: pd.DataFrame, column: str, fill_nulls: bool) -> np.ndarray:
    """Pick a column from the DataFrame, optionally replacing null values with a predefined value."""
    series = df[column]
    if fill_nulls:
        try:
            replacement = _NULL_REPLACEMENTS[column]
        except KeyError:
            replacement = 0
            if df[column].isna().any():
                logger.warning(
                    "Column '%s' contains null values and no replacement value is set, filling with %r.",
                    column,
                    replacement,
                )
        series = series.fillna(replacement)
    return series.to_numpy().reshape(-1, 1)  # 2D columnar array


def create_classifier(embedding_dimension: int, random_state: int | None = None) -> sklearn.pipeline.Pipeline:
    """Create a classifier pipeline for document type classification.

    The input to this pipeline is generated by the :func:`create_matrices` function.
    """
    i_embeddings = 0
    i_extra_features = i_embeddings + embedding_dimension
    i_categorical_features = i_extra_features + len(EXTRA_FEATURE_COLUMNS)

    pipeline = sklearn.pipeline.make_pipeline(
        ColumnTransformer(
            [
                (
                    "embeddings",
                    sklearn.pipeline.make_pipeline(
                        StandardScaler(),
                        PCA(n_components=40, random_state=random_state),
                    ),
                    slice(i_embeddings, i_extra_features),
                ),
                (
                    "extra_features",
                    sklearn.pipeline.make_pipeline(
                        StandardScaler(),
                    ),
                    slice(i_extra_features, i_categorical_features),
                ),
                (
                    "categorical_features",
                    sklearn.pipeline.make_pipeline(
                        # OneHotEncoder(
                        #     sparse_output=False,
                        #     categories=[
                        #         # list(schemata.CANTON_CODES | {schemata.FEDERAL_CODE}),
                        #         # ["fedlex", "openparldata"],
                        #     ],
                        # ),
                        StandardScaler(),
                    ),
                    slice(i_categorical_features, None),
                ),
            ]
        ),
        # LogisticRegression(max_iter=2000),
        RandomForestClassifier(random_state=random_state),
        # GradientBoostingClassifier(random_state=RANDOM_STATE),
    )
    return pipeline


def get_pca_step(pipeline: sklearn.pipeline.Pipeline) -> PCA | None:
    """Get the PCA step from the pipeline generated by :func:`create_classifier`, if present."""
    try:
        return pipeline.named_steps["columntransformer"].named_transformers_["embeddings"].named_steps["pca"]
    except (KeyError, AttributeError):
        return None
