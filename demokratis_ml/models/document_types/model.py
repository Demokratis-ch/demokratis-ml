"""Model construction for document type classification."""

import logging
from typing import Any

import numpy as np
import pandas as pd
import sklearn.pipeline
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.ensemble import (
    GradientBoostingClassifier,  # noqa: F401
    RandomForestClassifier,
)
from sklearn.linear_model import LogisticRegression  # noqa: F401
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler  # noqa: F401
from sklearn.svm import SVC  # noqa: F401

EXTRA_FEATURE_COLUMNS = (
    # PDF features
    "count_pages",  # int: number of pages in the document
    "count_pages_containing_images",  # int: number of pages containing images (for documents <= 50 pages)
    "count_pages_containing_tables",  # int: number of pages containing tables (for documents <= 50 pages)
    "average_page_aspect_ratio",  # float: average aspect ratio of pages (width / height) (for documents <= 50 pages)
    # / Removed: train/test distributions don't match well:
    # \ "fraction_pages_containing_images",  # float: count_pages_containing_images / count_pages
    "fraction_pages_containing_tables",  # float: count_pages_containing_tables / count_pages
    # Keyword-like features
    "contains_synopse_keyword",  # bool: whether the beginning of the document contains the word "synopse"
    "contains_salutation",  # bool: whether the beginning of the document contains a formal German letter greetings
    "contains_table_on_first_page",  # bool: whether the first page contains a table
    # Time features
    "days_after_consultation_start",  # int: days since the consultation started
    "days_after_consultation_end",  # int: days since the consultation ended (often a negative number)
    "consultation_start_timestamp",  # int: timestamp of the consultation start date (in seconds since epoch)
)

EXTRA_CATEGORICAL_COLUMNS = ("is_federal_consultation",)

_NULL_REPLACEMENTS = {
    "count_pages": -1,
    "count_pages_containing_images": -1,
    "count_pages_containing_tables": -1,
    "average_page_aspect_ratio": -1.0,
    "fraction_pages_containing_tables": -1.0,
}

logger = logging.getLogger("document_types.model")


def create_matrices(df: pd.DataFrame, fill_nulls: bool = False) -> tuple[np.ndarray, pd.Series]:
    """Convert a dataframe (the result of preprocessing) into a feature matrix and a target vector.

    RandomForestClassifier natively supports nulls since sklearn 1.7, so we don't necessarily need
    to fill them.
    """
    embeddings = np.vstack(df["embedding"])
    x = np.hstack(
        [embeddings]
        + [_pick_column(df, column, fill_nulls) for column in EXTRA_FEATURE_COLUMNS]
        + [
            df[list(EXTRA_CATEGORICAL_COLUMNS)],
        ]
    ).astype(np.float32)
    y = df["document_type"]
    assert x.shape[0] == y.shape[0]
    return x, y


def _pick_column(df: pd.DataFrame, column: str, fill_nulls: bool) -> np.ndarray:
    """Pick a column from the DataFrame, optionally replacing null values with a predefined value."""
    series = df[column]
    if fill_nulls:
        try:
            replacement = _NULL_REPLACEMENTS[column]
        except KeyError:
            replacement = 0
            if df[column].isna().any():
                logger.warning(
                    "Column '%s' contains null values and no replacement value is set, filling with %r.",
                    column,
                    replacement,
                )
        series = series.fillna(replacement)
    return series.to_numpy().reshape(-1, 1)  # 2D columnar array


def create_classifier(
    embedding_dimension: int,
    params: dict[str, Any],
    random_state: int | None = None,
) -> sklearn.pipeline.Pipeline:
    """Create a classifier pipeline for document type classification.

    The input to this pipeline is generated by the :func:`create_matrices` function.
    """
    i_embeddings = 0
    i_extra_features = i_embeddings + embedding_dimension
    i_categorical_features = i_extra_features + len(EXTRA_FEATURE_COLUMNS)

    clf_params = params["classifier"]
    match clf_params["type"]:
        case "LogisticRegression":
            classifier = LogisticRegression(max_iter=2000, random_state=random_state)
        case "RandomForest":
            classifier = RandomForestClassifier(
                random_state=random_state,
                n_estimators=clf_params["n_estimators"],
                criterion=clf_params["criterion"],
                max_depth=clf_params["max_depth"],
                min_samples_split=clf_params["min_samples_split"],
                min_samples_leaf=clf_params["min_samples_leaf"],
                class_weight=clf_params["class_weight"],
            )
        case "MLP":
            classifier = MLPClassifier(random_state=random_state, hidden_layer_sizes=clf_params["hidden_layer_sizes"])
        case "SVC":
            classifier = SVC(
                random_state=random_state,
                C=clf_params["C"],
                kernel=clf_params["kernel"]["type"],
                class_weight=clf_params["class_weight"],
            )
        case _:
            raise ValueError("Unknown classifier type", params["classifier"]["type"])

    pipeline = sklearn.pipeline.make_pipeline(
        ColumnTransformer(
            [
                (
                    "embeddings",
                    sklearn.pipeline.make_pipeline(
                        StandardScaler(),
                        PCA(n_components=params["pca_n_components"], random_state=random_state),
                    ),
                    slice(i_embeddings, i_extra_features),
                ),
                (
                    "extra_features",
                    # "passthrough",
                    sklearn.pipeline.make_pipeline(
                        StandardScaler(),
                    ),
                    slice(i_extra_features, i_categorical_features),
                ),
                (
                    "categorical_features",
                    # "passthrough",
                    sklearn.pipeline.make_pipeline(
                        StandardScaler(),
                    ),
                    slice(i_categorical_features, None),
                ),
            ]
        ),
        classifier,
        # LogisticRegression(max_iter=2000),
        # RandomForestClassifier(random_state=random_state),
        # GradientBoostingClassifier(random_state=RANDOM_STATE),
        # MLPClassifier(random_state=random_state, hidden_layer_sizes=params["hidden_layer_sizes"]),
        # SVC(random_state=random_state),
    )
    return pipeline


def get_pca_step(pipeline: sklearn.pipeline.Pipeline) -> PCA | None:
    """Get the PCA step from the pipeline generated by :func:`create_classifier`, if present."""
    try:
        return pipeline.named_steps["columntransformer"].named_transformers_["embeddings"].named_steps["pca"]
    except (KeyError, AttributeError):
        return None
