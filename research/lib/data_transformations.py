from collections.abc import Iterable

import numpy as np
import pandas as pd
import pandera.pandas as pa
from pandera.typing import DataFrame
from sklearn.preprocessing import MultiLabelBinarizer

from demokratis_ml.data import schemata


@pa.check_types
def create_synthetic_documents_from_consultations(
    df_docs: DataFrame[schemata.FullConsultationDocumentSchemaV1],
) -> pd.DataFrame:
    # Can't return "DataFrame[schemata.FullConsultationDocumentSchemaV1]" because document_type
    # doesn't match the allowed document types.
    """
    Create "document" rows containing consultation metadata.

    This is a trick through which we include the consultation title, description, and the organisation name
    in input data, which is otherwise composed of consultation document *contents* only.
    """
    assert len(df_docs["document_language"].unique()) == 1
    df_consultations = (
        df_docs.groupby("consultation_identifier")
        .agg(
            {
                "consultation_start_date": "first",
                "consultation_end_date": "first",
                "consultation_title": "first",
                "consultation_description": "first",
                "consultation_url": "first",
                "consultation_topics": "first",
                "consultation_topics_label_source": "first",
                "consultation_internal_tags": "first",
                "organisation_uuid": "first",
                "organisation_name": "first",
                "political_body": "first",
                "document_uuid": lambda _: -1,
                "document_source": "first",
                "document_source_url": "first",
                # document_type
                "document_language": "first",  # See the assertion above
                "document_title": lambda _: None,
            }
        )
        .reset_index()
    )

    # Define the columns to stack into the new `document_content_plain` column
    columns_to_stack = [
        "organisation_name",
        "consultation_title",
        "consultation_description",
    ]
    # Melt the DataFrame to create rows for each of the `columns_to_stack`
    df_consultations_synthetic = df_consultations.melt(
        id_vars=[col for col in df_consultations.columns if col not in columns_to_stack],
        value_vars=columns_to_stack,
        var_name="document_type",
        value_name="document_content_plain",
    )
    assert len(df_consultations_synthetic) == len(df_consultations) * len(columns_to_stack)

    df_consultations_synthetic["document_content_plain"] = df_consultations_synthetic[
        "document_content_plain"
    ].str.strip()
    for empty_col in columns_to_stack:
        df_consultations_synthetic[empty_col] = "<synthetic>"
    df_consultations_synthetic["document_type"] = df_consultations_synthetic["document_type"].astype("category")
    return df_consultations_synthetic


def create_input_dataframe(
    *document_dfs: pd.DataFrame,
) -> tuple[pd.DataFrame, list[str]]:
    """Concatenate multiple input dataframes with documents and one-hot encode the topics.

    The multiple dataframes might be e.g. "real" documents and synthetic ones
    generated by :func:`create_synthetic_documents_from_consultations`.

    :returns: The resulting dataframe and the list of topic columns
    (e.g. ["topic_social", "topic_insurance", ...]).
    """
    df_input = pd.concat(document_dfs, ignore_index=True)

    to_drop = df_input["document_content_plain"] == ""
    print(f"Dropping {len(df_input[to_drop])} documents with empty text.")
    df_input = df_input[~to_drop]

    topic_binarizer = MultiLabelBinarizer()
    one_hot_labels = pd.DataFrame(
        topic_binarizer.fit_transform(df_input["consultation_topics"]),
        columns=[f"topic_{c}" for c in topic_binarizer.classes_],
        index=df_input.index,
    )
    topic_columns = one_hot_labels.columns
    df_input = pd.concat([df_input, one_hot_labels], axis=1)
    return df_input, topic_columns.tolist()


def drop_underrepresented_topics(
    df_input: pd.DataFrame,
    topic_columns: Iterable[str],
    min_consultations_in_class: int,
    *,
    always_drop_topics: Iterable[str] = (),
) -> tuple[pd.DataFrame, list[str]]:
    """Drop topics that are not represented in enough samples (documents).

    Also drops documents that no longer have any labels after dropping the under-represented topics.
    """
    # Drop columns
    always_drop_topics = {t if t.startswith("topic_") else f"topic_{t}" for t in always_drop_topics}
    if always_drop_topics - set(topic_columns):
        raise ValueError(
            "The following topics are not present in the input data", always_drop_topics - set(topic_columns)
        )
    consultations_per_topic = df_input.groupby("consultation_identifier").agg({c: "first" for c in topic_columns}).sum()
    to_drop = consultations_per_topic[
        (consultations_per_topic < min_consultations_in_class)
        | (consultations_per_topic.index.isin(always_drop_topics))
    ]
    print("Dropping these underrepresented classes:\n", to_drop)
    df_input = df_input.drop(columns=to_drop.index)
    topic_columns = [c for c in topic_columns if c not in to_drop.index]
    # Drop rows that no longer have any labels
    documents_without_label = df_input[topic_columns].sum(axis=1) == 0
    print(
        "Dropping these documents without any label:",
        len(df_input[documents_without_label]),
    )
    df_input = df_input[~documents_without_label]

    return df_input, topic_columns


def group_document_labels_by_consultation(
    consultation_identifiers: pd.Series,
    label_names: list[str] | tuple[str, ...] | pd.Index,
    doc_labels: np.ndarray | pd.DataFrame,
    threshold: float = 0.333,
) -> pd.DataFrame:
    """When documents are labelled individually (e.g. when labels are known, or when they're predicted
    by a model), we want to combine these labels to get a single label for each consultation.
    (Recall that each consultation can have multiple documents.)

    This function groups document labels by consultation, and then votes on the labels for each
    consultation. If more than (threshold * 100)% of documents vote for a given label, that label
    is assigned to the consultation.
    """
    if isinstance(doc_labels, pd.DataFrame):
        doc_labels = doc_labels.to_numpy()
    # The code would run but it'd produce nonsense if a Pandas dataframe was passed instead.
    assert isinstance(doc_labels, np.ndarray)
    assert consultation_identifiers.size == doc_labels.shape[0]
    assert len(label_names) == doc_labels.shape[1]
    assert 0 < threshold <= 1

    df_docs = pd.DataFrame(doc_labels, columns=label_names)
    df_docs["consultation_identifier"] = consultation_identifiers.reset_index(drop=True)

    def vote(doc_labels: pd.Series) -> pd.Series:
        return (doc_labels.sum() > doc_labels.size * threshold).astype(int)

    df_consultation_labels = df_docs.groupby("consultation_identifier").agg(vote)
    return df_consultation_labels
