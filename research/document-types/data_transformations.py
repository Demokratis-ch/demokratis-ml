from collections.abc import Iterable

import numpy as np
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer


def create_synthetic_documents_from_consultations(
    df_docs: pd.DataFrame,
) -> pd.DataFrame:
    assert len(df_docs["document_language"].unique()) == 1
    df_consultations = (
        df_docs.groupby("consultation_id")
        .agg(
            {
                "document_source": "first",
                "consultation_topics_label_source": "first",
                "document_id": lambda _: -1,
                "consultation_title": "first",
                "consultation_description": "first",
                "consultation_url": "first",
                "consultation_topics": "first",
                "document_language": "first",  # See the assertion above
                "organisation_id": "first",
                "organisation_name": "first",
            }
        )
        .reset_index()
    )

    # Define the columns to stack into the new `doc_text_repr` column
    columns_to_stack = [
        "organisation_name",
        "consultation_title",
        "consultation_description",
    ]
    # Melt the DataFrame to create rows for each of the `columns_to_stack`
    df_consultations_synthetic = df_consultations.melt(
        id_vars=[col for col in df_consultations.columns if col not in columns_to_stack],
        value_vars=columns_to_stack,
        var_name="document_type",
        value_name="doc_text_repr",
    )
    assert len(df_consultations_synthetic) == len(df_consultations) * len(columns_to_stack)

    df_consultations_synthetic["doc_text_repr"] = df_consultations_synthetic["doc_text_repr"].str.strip()
    return df_consultations_synthetic


def create_input_dataframe(
    *document_dfs: pd.DataFrame,
) -> tuple[pd.DataFrame, list[str]]:
    """Concatenate multiple input dataframes with documents (e.g. "real" documents and synthetic ones
    generated by :func:`create_synthetic_documents_from_consultations`) and one-hot encode the topics.
    Return the resulting dataframe and the list of topic columns
    (e.g. ["topic_social", "topic_insurance", ...]).
    """
    df_input = pd.concat(document_dfs, ignore_index=True)

    to_drop = df_input["doc_text_repr"] == ""
    print(f"Dropping {len(df_input[to_drop])} documents with empty text.")
    df_input = df_input[~to_drop]

    topic_binarizer = MultiLabelBinarizer()
    one_hot_labels = pd.DataFrame(
        topic_binarizer.fit_transform(df_input["consultation_topics"]),
        columns=[f"topic_{c}" for c in topic_binarizer.classes_],
        index=df_input.index,
    )
    topic_columns = one_hot_labels.columns
    df_input = pd.concat([df_input, one_hot_labels], axis=1)
    return df_input, topic_columns.tolist()


def drop_underrepresented_topics(
    df_input: pd.DataFrame,
    topic_columns: Iterable[str],
    min_consultations_in_class: int,
    *,
    always_drop_topics: Iterable[str] = (),
) -> tuple[pd.DataFrame, list[str]]:
    # Drop columns
    always_drop_topics = {t if t.startswith("topic_") else f"topic_{t}" for t in always_drop_topics}
    if always_drop_topics - set(topic_columns):
        raise ValueError(
            "The following topics are not present in the input data", always_drop_topics - set(topic_columns)
        )
    consultations_per_topic = df_input.groupby("consultation_id").agg({c: "first" for c in topic_columns}).sum()
    to_drop = consultations_per_topic[
        (consultations_per_topic < min_consultations_in_class)
        | (consultations_per_topic.index.isin(always_drop_topics))
    ]
    print("Dropping these underrepresented classes:\n", to_drop)
    df_input = df_input.drop(columns=to_drop.index)
    topic_columns = [c for c in topic_columns if c not in to_drop.index]
    # Drop rows that no longer have any labels
    documents_without_label = df_input[topic_columns].sum(axis=1) == 0
    print(
        "Dropping these documents without any label:",
        len(df_input[documents_without_label]),
    )
    df_input = df_input[~documents_without_label]

    return df_input, topic_columns


def group_document_labels_by_consultation(
    consultation_ids: pd.Series,
    label_names: list[str] | tuple[str, ...] | pd.Index,
    doc_labels: np.ndarray | pd.DataFrame,
    threshold: float = 0.333,
) -> pd.DataFrame:
    """When documents are labelled individually (e.g. when labels are known, or when they're predicted
    by a model), we want to combine these labels to get a single label for each consultation.
    (Recall that each consultation can have multiple documents.)

    This function groups document labels by consultation, and then votes on the labels for each
    consultation. If more than (threshold * 100)% of documents vote for a given label, that label
    is assigned to the consultation.
    """
    if isinstance(doc_labels, pd.DataFrame):
        doc_labels = doc_labels.values
    # The code runs but produces nonsense when a Pandas dataframe is passed instead.
    assert isinstance(doc_labels, np.ndarray)
    assert consultation_ids.size == doc_labels.shape[0]
    assert len(label_names) == doc_labels.shape[1]
    assert 0 < threshold <= 1

    df_docs = pd.DataFrame(doc_labels, columns=label_names)
    df_docs["consultation_id"] = consultation_ids.reset_index(drop=True)

    def vote(doc_labels: pd.Series) -> pd.Series:
        return (doc_labels.sum() > doc_labels.size * threshold).astype(int)

    df_consultation_labels = df_docs.groupby("consultation_id").agg(vote)
    return df_consultation_labels
